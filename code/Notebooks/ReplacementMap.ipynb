{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replacment of `tf.constant`**\n",
    "\n",
    "`tf.constant` Creates a [constan tensor](https://www.tensorflow.org/api_docs/python/tf/compat/v1/constant) that can be replaced with simple `torch.Tensor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_embeds_t = torch.Tensor([[0.4, 0.3],[0.3, 0.2]])\n",
    "with tf.variable_scope('name_view' + 'embeddings'):\n",
    "    name_embeds = tf.constant([[0.4, 0.3],[0.3, 0.2]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"name_viewembeddings_2/Const:0\", shape=(2, 2), dtype=float32) tensor([[0.4000, 0.3000],\n",
      "        [0.3000, 0.2000]])\n"
     ]
    }
   ],
   "source": [
    "print(name_embeds, name_embeds_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replacement of `xavier_init` defined by `MultiKE`**\n",
    "\n",
    "that uses `tf.variable_scope` which is a context manager for defining ops that creates variables (layers).\n",
    "\n",
    "The following means that a layer is supposed to be initialized with the [specified](http://tensorflow.biotecan.com/python/Python_1.8/tensorflow.google.cn/api_docs/python/tf/contrib/layers/xavier_initializer.html) initializer and if the corresponding param is set, normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(shape, name, is_l2_norm, dtype=None):\n",
    "    with tf.name_scope('xavier_init'):\n",
    "        embeddings = tf.get_variable(name, shape=shape, dtype=dtype,\n",
    "                                     initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n",
    "    return tf.nn.l2_normalize(embeddings, 1) if is_l2_norm else embeddings\n",
    "\n",
    "# https://docs.w3cub.com/tensorflow~python/tf/nn/l2_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable relation_viewembeddings/rv_ent_embeds already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/home/kiril/miniconda3/envs/labenv37/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"/home/kiril/miniconda3/envs/labenv37/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/home/kiril/miniconda3/envs/labenv37/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/home/kiril/miniconda3/envs/labenv37/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/kiril/miniconda3/envs/labenv37/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-aa4c4793ce47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# that is supposed to run once and then raise exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relation_view'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'embeddings'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrv_ent_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxavier_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m179\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m75\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rv_ent_embeds'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-a32fb8538586>\u001b[0m in \u001b[0;36mxavier_init\u001b[0;34m(shape, name, is_l2_norm, dtype)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xavier_init'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         embeddings = tf.get_variable(name, shape=shape, dtype=dtype,\n\u001b[0;32m----> 4\u001b[0;31m                                      initializer=tf.contrib.layers.xavier_initializer(uniform=False))\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_l2_norm\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/labenv37/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1498\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/labenv37/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1241\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/miniconda3/envs/labenv37/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    565\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/miniconda3/envs/labenv37/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    517\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     synchronization, aggregation, trainable = (\n",
      "\u001b[0;32m~/miniconda3/envs/labenv37/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[0;32m--> 868\u001b[0;31m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    869\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable relation_viewembeddings/rv_ent_embeds already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/home/kiril/miniconda3/envs/labenv37/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"/home/kiril/miniconda3/envs/labenv37/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/home/kiril/miniconda3/envs/labenv37/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/home/kiril/miniconda3/envs/labenv37/lib/python3.7/site-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/home/kiril/miniconda3/envs/labenv37/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "# that is supposed to run once and then raise ValueError\n",
    "with tf.variable_scope('relation_view' + 'embeddings'):\n",
    "    rv_ent_embeds = xavier_init([179, 75], 'rv_ent_embeds', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The xavier above has `uniform = False` which means the initialiser will use noral distribution. Below `xavier_normal_` is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, is_l2_norm=False):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = torch.nn.Parameter(torch.Tensor(out_features, in_features)) # linear layer\n",
    "        if(is_l2_norm): # check if it must be normalized.\n",
    "            self.weight = nn.functional.normalize(self.weight, dim=1, p=2) # do so\n",
    "        # assuming that this if-else is obsolete:\n",
    "        if bias: # see https://discuss.pytorch.org/t/tensorflow-converging-faster-than-pytorch/85640\n",
    "            self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_normal_(self.weight) # https://pytorch.org/docs/stable/nn.init.html\n",
    "        if self.bias is not None:\n",
    "            torch.nn.init.uniform_(self.bias, -1, 1)\n",
    "    def forward(self, input):\n",
    "        return torch.nn.functional.linear(input, self.weight, self.bias)\n",
    "        # or one can do a look-up for the embedding here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the following links\n",
    " - [big code snippets](https://discuss.pytorch.org/t/tensorflow-converging-faster-than-pytorch/85640)\n",
    " - [small code snippet](https://discuss.pytorch.org/t/how-to-initialize-the-conv-layers-with-xavier-weights-initialization/8419)\n",
    " - [l2 normalizer replacement](https://discuss.pytorch.org/t/how-to-implement-batch-l2-normalization-with-pytorch/39707)\n",
    " - [tf xavier doc](http://tensorflow.biotecan.com/python/Python_1.8/tensorflow.google.cn/api_docs/python/tf/contrib/layers/xavier_initializer.html)\n",
    " - [torch xavier doc](https://pytorch.org/docs/stable/nn.init.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Linear(179, 95, bias = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replacement for `tf.get_variable`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('shared' + 'combination'):\n",
    "            self.nv_mapping = tf.get_variable('nv_mapping', shape=[self.args.dim, self.args.dim],\n",
    "                                              initializer=tf.initializers.orthogonal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - [short example with comment for autograd](https://discuss.pytorch.org/t/tensorflow-get-variable-into-pytorch/83456)\n",
    " - as above, but with simplier code and initialzier is orthogonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharEmbeddings(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        # TODO before paste\n",
    "        super(CharEmbeddings, self).__init__()\n",
    "        \n",
    "        emb = torch.empty(in_features, out_features)\n",
    "        nn.init.orthogonal_(emb)\n",
    "        print(emb)\n",
    "        self.nv_mapping = nn.Parameter(emb)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.nv_mapping[input] # as mentioned in the analogous snippet above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5650, -0.2847, -0.0924, -0.7689],\n",
      "        [ 0.1821, -0.1216,  0.9543, -0.2034],\n",
      "        [-0.7954,  0.0284,  0.2708,  0.5414],\n",
      "        [-0.1222,  0.9504,  0.0863, -0.2725]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.5650, -0.2847, -0.0924, -0.7689], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CharEmbeddings(4,4).forward(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`tf.placeholder` replacement**\n",
    "\n",
    "`tf.placeholder` is not needed in torch as one can pass tensors to modules.\n",
    "\n",
    "**`tf.embedding_lookup` replacement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('relation_triple_placeholder'):\n",
    "    rel_pos_hs = tf.placeholder(tf.int32, shape=[None])\n",
    "        \n",
    "with tf.name_scope('relation_triple_lookup'):\n",
    "    rel_phs = tf.nn.embedding_lookup(rv_ent_embeds, rel_pos_hs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either or:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharEmbeddings(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        # TODO before paste\n",
    "        super(CharEmbeddings, self).__init__()\n",
    "        \n",
    "        emb = torch.empty(in_features, out_features)\n",
    "        nn.init.orthogonal_(emb)\n",
    "        print(emb)\n",
    "        self.nv_mapping = nn.Parameter(emb)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.nv_mapping[input] # as mentioned in the analogous snippet above.\n",
    "\n",
    "emb = nn.Embedding(10, 100)\n",
    "x = torch.tensor([1,2])\n",
    "out = emb(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2863,  0.7333, -0.6096, -1.9199, -2.1164,  0.1582,  0.2363, -3.3401,\n",
       "         -0.7276,  1.0803, -0.8376,  0.8346, -0.5104, -1.1642, -1.5005,  0.4684,\n",
       "          0.1012,  1.5001, -0.8946,  0.1354, -1.5158,  0.9176, -2.0890,  3.2442,\n",
       "         -0.0109, -0.2588, -1.6685, -0.9931,  0.2125, -0.3699, -0.3966, -1.3968,\n",
       "         -0.0060, -1.2941, -0.7010,  2.1114, -0.5782, -0.6943,  0.8156,  1.4890,\n",
       "          0.0225, -2.1523,  0.1722,  0.4665,  0.1411,  0.6178, -0.9796,  1.0282,\n",
       "         -0.6518, -0.7431,  0.4624,  0.9266,  0.2300, -0.3015,  0.2870,  1.1188,\n",
       "         -0.2594,  0.6772,  0.2805,  0.7594,  0.9710,  0.8420, -0.2592, -1.0879,\n",
       "         -1.1377, -0.0314, -0.2035, -0.6956, -0.9920,  0.8346,  1.1025,  0.9616,\n",
       "         -0.2370, -0.0431,  0.1949,  0.0224, -1.9532,  0.2481, -0.0801, -1.1726,\n",
       "         -1.2579,  0.2772,  0.2767, -1.2489,  0.1103, -0.7892,  2.6883,  0.5771,\n",
       "          2.3581,  0.1723,  0.6556, -0.5036, -0.3731, -0.3223, -0.7230,  0.1607,\n",
       "          1.4014, -0.3316,  0.6052, -0.2639]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([0])\n",
    "emb(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3627,  0.4402],\n",
      "        [ 0.0935, -0.1135]])\n"
     ]
    }
   ],
   "source": [
    "#Example\n",
    "def my_loss(output, target):\n",
    "    loss = torch.mean((output - target)**2)\n",
    "    return loss\n",
    "\n",
    "model = nn.Linear(2, 2)\n",
    "x = torch.randn(1, 2)\n",
    "target = torch.randn(1, 2)\n",
    "output = model(x)\n",
    "loss = my_loss(output, target)\n",
    "loss.backward()\n",
    "print(model.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relation_logistic_loss(phs, prs, pts, nhs, nrs, nts):\n",
    "    pos_distance = phs + prs - pts\n",
    "    neg_distance = nhs + nrs - nts\n",
    "    pos_score = -tf.reduce_sum(tf.square(pos_distance), axis=1)\n",
    "    neg_score = -tf.reduce_sum(tf.square(neg_distance), axis=1)\n",
    "    pos_loss = tf.reduce_sum(tf.log(1 + tf.exp(-pos_score)))\n",
    "    neg_loss = tf.reduce_sum(tf.log(1 + tf.exp(neg_score)))\n",
    "    loss = tf.add(pos_loss, neg_loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(phs, prs, pts, nhs, nrs, nts):\n",
    "    pos_distance = phs + prs - pts\n",
    "    neg_distance = nhs + nrs - nts\n",
    "    pos_score = -torch.sum(torch.square(pos_distance), dim=1)\n",
    "    neg_score = -torch.sum(torch.square(neg_distance), dim=1)\n",
    "    pos_loss = torch.sum(torch.log(1 + torch.exp(-pos_score)))\n",
    "    neg_loss = torch.sum(torch.log(1 + torch.exp(neg_score)))\n",
    "    loss = torch.add(pos_loss, neg_loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
